{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6d952635",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image\n",
    "import re\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.compat.v1.train import Optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e66fe51",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 95000 images with corresponding font names.\n",
      "\n",
      "Fonts:\n",
      "0: Century\n",
      "1: Futura\n",
      "2: Calligraphy\n",
      "3: Candara\n",
      "4: Corbel\n",
      "5: LCD Mono\n",
      "6: Bembo\n",
      "7: Myriad\n",
      "8: Georgia\n",
      "9: Didot\n",
      "10: Bell MT\n",
      "11: Hombre\n",
      "12: Garamond\n",
      "13: Mrs Eaves\n",
      "14: Cambria\n",
      "15: Algerian\n",
      "16: Monotype Corsiva\n",
      "17: Book Antiqua\n",
      "18: Comic Sans MS\n",
      "19: Brandish\n",
      "20: Consolas\n",
      "21: Lucida Bright\n",
      "22: Courier\n",
      "23: Calvin\n",
      "24: Californian FB\n",
      "25: Bodoni\n",
      "26: Akzidenz Grotesk\n",
      "27: Gill sans\n",
      "28: Minion\n",
      "29: Calibry\n",
      "30: Franklin Gothic\n",
      "31: Fascinate\n",
      "32: Baskerville\n",
      "33: Agency\n",
      "34: Frutiger\n",
      "35: Helvetica\n",
      "36: Arial\n",
      "37: Elephant\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def load_font_data(data_dir):\n",
    "\n",
    "    \n",
    "\n",
    "    image_font_pairs = []\n",
    "\n",
    "    # Loop through each subdirectory (font folder)\n",
    "    for font_dir in os.listdir(data_dir):\n",
    "        if os.path.isdir(os.path.join(data_dir, font_dir)):\n",
    "            # Get the font name\n",
    "            font_name = font_dir\n",
    "\n",
    "            # Get all image files in the font directory\n",
    "            image_files = [f for f in os.listdir(os.path.join(data_dir, font_dir)) if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "\n",
    "            # Select every 2nd image starting from the first one\n",
    "            for i in range(0, len(image_files), 2):\n",
    "                # Build the image path\n",
    "                img_path = os.path.join(data_dir, font_dir, image_files[i])\n",
    "\n",
    "                # Append (image_path, font_name) tuple to the list\n",
    "                image_font_pairs.append((img_path, font_name))\n",
    "\n",
    "    return image_font_pairs\n",
    "\n",
    "# Example usage\n",
    "data_dir = r\"C:\\Users\\prabh_6tzckcr\\Downloads\\Font Dataset Large\"\n",
    "image_font_pairs = load_font_data(data_dir)\n",
    "\n",
    "print(f\"Loaded {len(image_font_pairs)} images with corresponding font names.\")\n",
    "\n",
    "# Show the actual font names\n",
    "output_labels = set([pair[1] for pair in image_font_pairs])\n",
    "\n",
    "print(\"\\nFonts:\")\n",
    "for label, font_name in enumerate(output_labels):\n",
    "    print(f\"{label}: {font_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546665b2",
   "metadata": {},
   "source": [
    "This function loads font data from a directory containing font images. It traverses each subdirectory within the provided data_dir, assuming each subdirectory represents a font, and collects pairs of image paths and corresponding font names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7cbe4c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed 12000 images with corresponding font names.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_images(image_font_pairs, target_size=(64, 64)):\n",
    "    \"\"\"\n",
    "    Preprocesses a list of images by loading, converting to grayscale, resizing, and normalizing.\"\"\"\n",
    "\n",
    "    preprocessed_images = []\n",
    "    font_names = []\n",
    "\n",
    "    # Randomly select 12000 image-font pairs\n",
    "    image_font_pairs_subset = random.sample(image_font_pairs, 12000)\n",
    "\n",
    "    for img_path, font_name in image_font_pairs_subset:\n",
    "        try:\n",
    "            # Load the image\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "            # Check if the image is loaded successfully\n",
    "            if img is None:\n",
    "                print(f\"Error loading image: {img_path}\")\n",
    "                continue\n",
    "\n",
    "            # Resize the image while preserving aspect ratio\n",
    "            height, width = img.shape\n",
    "            if height > width:\n",
    "                ratio = target_size[0] / height\n",
    "                new_height = target_size[0]\n",
    "                new_width = int(width * ratio)\n",
    "            else:\n",
    "                ratio = target_size[1] / width\n",
    "                new_width = target_size[1]\n",
    "                new_height = int(height * ratio)\n",
    "            img = cv2.resize(img, (new_width, new_height))\n",
    "\n",
    "            # Add padding if necessary to match the target size\n",
    "            pad_height = target_size[0] - new_height\n",
    "            pad_width = target_size[1] - new_width\n",
    "            top = pad_height // 2\n",
    "            bottom = pad_height - top\n",
    "            left = pad_width // 2\n",
    "            right = pad_width - left\n",
    "            img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=255)\n",
    "\n",
    "            # Convert the image to binary format for better feature extraction\n",
    "            _, img = cv2.threshold(img, 128, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "\n",
    "            # Normalize the pixel values to the range [0, 1]\n",
    "            img = img / 255.0\n",
    "\n",
    "            # Add a channel dimension to make it compatible with neural network input shape\n",
    "            img = np.expand_dims(img, axis=-1)\n",
    "\n",
    "            # Append preprocessed image and font name to the lists\n",
    "            preprocessed_images.append(img)\n",
    "            font_names.append(font_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image: {img_path} - {e}\")\n",
    "\n",
    "    return preprocessed_images, font_names\n",
    "\n",
    "# Example usage\n",
    "preprocessed_images, font_names = preprocess_images(image_font_pairs)\n",
    "\n",
    "print(f\"Preprocessed {len(preprocessed_images)} images with corresponding font names.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd46daf0",
   "metadata": {},
   "source": [
    "This function preprocesses a list of images by performing the following steps:\n",
    "\n",
    "Loading the image  \n",
    "\n",
    "Converting to grayscale  \n",
    "\n",
    "Resizing with aspect ratio preservation  \n",
    "\n",
    "Adding padding if necessary to match the target size  \n",
    "  \n",
    "Converting to binary format for better feature extraction  \n",
    "\n",
    "Normalizing pixel values to the range [0, 1]  \n",
    "\n",
    "Adding a channel dimension to make it compatible with neural network input shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bc707593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_12 (Conv2D)          (None, 62, 62, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPooli  (None, 31, 31, 32)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 29, 29, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_13 (MaxPooli  (None, 14, 14, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 12, 12, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPooli  (None, 6, 6, 128)         0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 4, 4, 256)         295168    \n",
      "                                                                 \n",
      " max_pooling2d_15 (MaxPooli  (None, 2, 2, 256)         0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 40)                20520     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 933160 (3.56 MB)\n",
      "Trainable params: 933160 (3.56 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def create_cnn_model(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Creates a convolutional neural network (CNN) model with four convolutional layers.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    model = models.Sequential()\n",
    "\n",
    "    # Convolutional layer 1\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    # Convolutional layer 2\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    # Convolutional layer 3\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    # Convolutional layer 4\n",
    "    model.add(layers.Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "    # Flatten layer\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    # Fully connected layer\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "\n",
    "    # Dropout regularization\n",
    "    model.add(layers.Dropout(0.5))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define input shape and number of classes\n",
    "input_shape = (64, 64, 1)  \n",
    "num_classes = 40 # 40 different font classes\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Create CNN model\n",
    "cnn_model = create_cnn_model(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "cnn_model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "cnn_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7038cc3",
   "metadata": {},
   "source": [
    "This function creates a convolutional neural network (CNN) model with four convolutional layers followed by fully connected layers and an output layer for classification.\n",
    "\n",
    "Arguments:input_shape (tuple): The shape of the input images (height, width, channels).\n",
    "num_classes (int): The number of classes (i.e., font classes) for classification.  \n",
    "\n",
    "Returns:tf.keras.Model: The CNN model. \n",
    "    \n",
    "Model Summary\n",
    "The created CNN model consists of four convolutional layers with max pooling, followed by a flatten layer, a fully connected layer with ReLU activation, dropout regularization, and an output layer with softmax activation.\n",
    "The input shape is (64, 64, 1) indicating grayscale images of size 64x64 pixels.\n",
    "The model is compiled using the Adam optimizer with sparse categorical crossentropy loss and accuracy as the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c611c790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 8400 images\n",
      "Validation set: 1800 images\n",
      "Test set: 1800 images\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and temp sets (70% training, 30% temp)\n",
    "train_images, temp_images, train_fonts, temp_fonts = train_test_split(preprocessed_images, font_names, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split temp set into validation and test sets (50% validation, 50% test)\n",
    "val_images, test_images, val_fonts, test_fonts = train_test_split(temp_images, temp_fonts, test_size=0.5, random_state=0)\n",
    "\n",
    "# Print sizes of each set\n",
    "print(f\"Training set: {len(train_images)} images\")\n",
    "print(f\"Validation set: {len(val_images)} images\")\n",
    "print(f\"Test set: {len(test_images)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935e807f",
   "metadata": {},
   "source": [
    "This code snippet splits the preprocessed images and their corresponding font names into training, validation, and test sets using the train_test_split function from scikit-learn.\n",
    "\n",
    "The initial split is performed with a test size of 30%, resulting in 70% of the data used for training and 30% for further splitting.\n",
    "\n",
    "The temp set obtained from the initial split is further split into validation and test sets with a test size of 50% each, resulting in equal proportions for validation and test sets.\n",
    "\n",
    "The random_state parameter ensures reproducibility by fixing the random seed for the splitting process.\n",
    "\n",
    "Output\n",
    "\n",
    "After splitting, the code prints the sizes of each set, including the number of images in the training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "35922e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform labels for training data\n",
    "train_labels_encoded = label_encoder.fit_transform(train_fonts)\n",
    "\n",
    "# Transform labels for validation and test data\n",
    "val_labels_encoded = label_encoder.transform(val_fonts)\n",
    "test_labels_encoded = label_encoder.transform(test_fonts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9182813b",
   "metadata": {},
   "source": [
    "\n",
    "The fit_transform method is used on the training font names (train_fonts) to both fit the encoder to the unique font names and transform them into numerical labels (train_labels_encoded). \n",
    "\n",
    "For the validation and test sets, the transform method is used to transform the font names into corresponding numerical labels without refitting the encoder. \n",
    "\n",
    "Output\n",
    "The encoded labels for the training, validation, and test sets are stored in the variables train_labels_encoded, val_labels_encoded, and test_labels_encoded, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "63427386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train_images, val_images, test_images to NumPy arrays\n",
    "train_images_np = np.array(train_images)\n",
    "val_images_np = np.array(val_images)\n",
    "test_images_np = np.array(test_images)\n",
    "\n",
    "# Convert train_fonts, val_fonts, test_fonts to NumPy arrays\n",
    "train_fonts_np = np.array(train_labels_encoded)\n",
    "val_fonts_np = np.array(val_labels_encoded)\n",
    "test_fonts_np = np.array(test_labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "716c614f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "263/263 [==============================] - 56s 197ms/step - loss: 3.5067 - accuracy: 0.0500 - val_loss: 2.9988 - val_accuracy: 0.1283\n",
      "Epoch 2/20\n",
      "263/263 [==============================] - 49s 186ms/step - loss: 2.8503 - accuracy: 0.1437 - val_loss: 2.5252 - val_accuracy: 0.2122\n",
      "Epoch 3/20\n",
      "263/263 [==============================] - 50s 188ms/step - loss: 2.4403 - accuracy: 0.2454 - val_loss: 2.0929 - val_accuracy: 0.3239\n",
      "Epoch 4/20\n",
      "263/263 [==============================] - 52s 199ms/step - loss: 2.0589 - accuracy: 0.3461 - val_loss: 1.7376 - val_accuracy: 0.4444\n",
      "Epoch 5/20\n",
      "263/263 [==============================] - 49s 185ms/step - loss: 1.7941 - accuracy: 0.4187 - val_loss: 1.7008 - val_accuracy: 0.4539\n",
      "Epoch 6/20\n",
      "263/263 [==============================] - 56s 214ms/step - loss: 1.6092 - accuracy: 0.4663 - val_loss: 1.5306 - val_accuracy: 0.4994\n",
      "Epoch 7/20\n",
      "263/263 [==============================] - 81s 305ms/step - loss: 1.4729 - accuracy: 0.5043 - val_loss: 1.4584 - val_accuracy: 0.5033\n",
      "Epoch 8/20\n",
      "263/263 [==============================] - 53s 201ms/step - loss: 1.3388 - accuracy: 0.5531 - val_loss: 1.4409 - val_accuracy: 0.5167\n",
      "Epoch 9/20\n",
      "263/263 [==============================] - 55s 208ms/step - loss: 1.2307 - accuracy: 0.5793 - val_loss: 1.3167 - val_accuracy: 0.5506\n",
      "Epoch 10/20\n",
      "263/263 [==============================] - 52s 199ms/step - loss: 1.1640 - accuracy: 0.5942 - val_loss: 1.2714 - val_accuracy: 0.5617\n",
      "Epoch 11/20\n",
      "263/263 [==============================] - 48s 183ms/step - loss: 1.0823 - accuracy: 0.6286 - val_loss: 1.2758 - val_accuracy: 0.5694\n",
      "Epoch 12/20\n",
      "263/263 [==============================] - 50s 190ms/step - loss: 1.0201 - accuracy: 0.6515 - val_loss: 1.2515 - val_accuracy: 0.5778\n",
      "Epoch 13/20\n",
      "263/263 [==============================] - 49s 188ms/step - loss: 0.9188 - accuracy: 0.6780 - val_loss: 1.1908 - val_accuracy: 0.6011\n",
      "Epoch 14/20\n",
      "263/263 [==============================] - 51s 194ms/step - loss: 0.8709 - accuracy: 0.6906 - val_loss: 1.1447 - val_accuracy: 0.6200\n",
      "Epoch 15/20\n",
      "263/263 [==============================] - 52s 193ms/step - loss: 0.8218 - accuracy: 0.7086 - val_loss: 1.2469 - val_accuracy: 0.5928\n",
      "Epoch 16/20\n",
      "263/263 [==============================] - 52s 196ms/step - loss: 0.7640 - accuracy: 0.7260 - val_loss: 1.1988 - val_accuracy: 0.6167\n",
      "57/57 [==============================] - 4s 63ms/step - loss: 1.1579 - accuracy: 0.6267\n",
      "Test Loss: 1.1578999757766724\n",
      "Test Accuracy: 0.6266666650772095\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
    "\n",
    "# Assuming your model is already defined\n",
    "\n",
    "# Prepare callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2)  # Stop training if val_loss doesn't improve for 2 epochs\n",
    "model_checkpoint = ModelCheckpoint(\"C:/Users/prabh_6tzckcr/Downloads/Keras/best_model.keras\", save_best_only=True)  # Save the best model based on val_loss\n",
    "\n",
    "\n",
    "# Train the model with callbacks\n",
    "history = cnn_model.fit(train_images_np, train_fonts_np, epochs=20,\n",
    "                        validation_data=(val_images_np, val_fonts_np),\n",
    "                        callbacks=[early_stopping, model_checkpoint])  # Pass callbacks as a list\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = cnn_model.evaluate(test_images_np, test_fonts_np)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "84105ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 4s 56ms/step\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      1.00      0.95        44\n",
      "           1       0.24      0.14      0.17        44\n",
      "           2       0.80      0.93      0.86        59\n",
      "           3       0.61      0.57      0.59        44\n",
      "           4       0.37      0.31      0.34        42\n",
      "           5       0.33      0.16      0.21        45\n",
      "           6       0.67      0.63      0.65        54\n",
      "           7       0.70      0.58      0.63        52\n",
      "           8       0.36      0.20      0.26        44\n",
      "           9       0.91      0.93      0.92        42\n",
      "          10       0.38      0.27      0.31        45\n",
      "          11       0.36      0.54      0.43        46\n",
      "          12       0.86      0.78      0.82        55\n",
      "          13       0.83      0.95      0.89        42\n",
      "          14       0.57      0.25      0.35        32\n",
      "          15       0.31      0.35      0.33        49\n",
      "          16       0.79      0.62      0.69        53\n",
      "          17       0.73      0.79      0.76        57\n",
      "          18       0.67      0.58      0.62        50\n",
      "          19       0.50      0.43      0.46        49\n",
      "          20       0.83      0.74      0.79        47\n",
      "          21       0.95      0.95      0.95        57\n",
      "          22       0.88      0.98      0.93        51\n",
      "          23       0.96      1.00      0.98        44\n",
      "          24       0.68      0.87      0.76        45\n",
      "          25       0.71      0.49      0.58        41\n",
      "          26       0.86      0.63      0.73        51\n",
      "          27       0.57      0.62      0.60        58\n",
      "          28       0.31      0.52      0.39        42\n",
      "          29       0.54      0.42      0.47        48\n",
      "          30       0.27      0.76      0.40        41\n",
      "          31       0.97      0.95      0.96        40\n",
      "          32       0.75      0.96      0.84        53\n",
      "          33       0.43      0.57      0.49        44\n",
      "          34       0.39      0.46      0.42        54\n",
      "          35       0.70      0.87      0.78        38\n",
      "          36       0.75      0.71      0.73        51\n",
      "          37       0.22      0.04      0.07        47\n",
      "\n",
      "    accuracy                           0.63      1800\n",
      "   macro avg       0.62      0.62      0.61      1800\n",
      "weighted avg       0.63      0.63      0.62      1800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Calculate predicted labels for the test set\n",
    "predicted_labels = np.argmax(cnn_model.predict(test_images_np), axis=1)\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(test_fonts_np, predicted_labels)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0afd52a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(\"C:/Users/prabh_6tzckcr/Downloads/Keras/best_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd485a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
